{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with GPT [GPT-2 small]\n",
    "   - We're going to use a pretrained model from Huggingface Transformers used mainly for Text Generation as it's based on 175 billion Parameters and got a very long Hours of training so that it has a very high Effeciency\n",
    "   - **Also the Main Reason of doing that bec our Model that we have created from scratch using RNNs and LSTMs has FAILED! :(**\n",
    "   - But the Bright Side is that Model will cost us only few lines of Code with very high accuarcy , so it's not that worse anymore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We've selected GPT-2 small \n",
    "   - you can see the diffrences between their types\n",
    "   - During the lack of time and limitation of Resources we choose small instead of GPT-2 large as it's much better than small\n",
    " \n",
    "<img src=https://jalammar.github.io/images/gpt2/gpt2-sizes-hyperparameters-3.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's CUDA\n",
    " - **CUDA(or Computer Unified Device Architecture)** is a proprietary parallel computing platform and programming model from NVIDIA. Using the CUDA SDK, developers can utilize their NVIDIA GPUs(Graphics Processing Units), thus enabling them to bring in the power of GPU-based parallel processing instead of the usual CPU-based sequential processing in their usual programming workflow. \n",
    "\n",
    "<img src=https://blogs.nvidia.com/wp-content/uploads/2012/09/cuda-apps-and-libraries.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "len(tf.config.list_physical_devices(\"GPU\")) ## PyTorch doesn't read CUDA so that i love TensorFlow ❤️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda_version) ## i don't know why it returns run .. tensorflow reads CUDA very Well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATE!!\n",
    "> Bec of unknown reasons Pytorch doesn't read CUDA so we'll use CPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook will not generate any useful file ,, it's only used for checking how would the Model work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Essential Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import string\n",
    "## we can use it for either Tensorflow or pytorch\n",
    "## for some Issues in my Device there is in compatiable Issue between Keras and Tensorflow Versions so unfortunately i can't use TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Generator\n",
    "> You need to download the Model in your Device .. since we're using the small one so it's only **512mb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Evaluating Result of this Model\n",
    "\n",
    "|Dataset |LAMBADA|LAMBADA |CBT-CN\t|CBT-NE\t|WikiText2 |PTB |enwiki8 |text8\t|WikiText103 |1BW |\n",
    "| ---- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "|(metric) |\t(PPL) |\t(ACC) |\t(ACC) |\t(ACC) |\t(PPL) |\t(PPL) |\t(BPB) |\t(BPC) |\t(PPL) |\t(PPL) |\n",
    "| |35.13|\t45.99|\t87.65|\t83.4|\t29.41|\t65.85|\t1.16|\t1,17|\t37.50 |75.20 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"what's the time now\"\n",
    "encoded_input = tokenizer.encode(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Decode Text \n",
    "> Breifly discuss what i'm going to do now \n",
    "   - i'll create a generator which will generate sentences with max isze of 100 words with activating of Early Stoping\n",
    "   - **Early Stoping** : Method in Deep Learning used to stop tarinig and excecuting any model when it begins to move from the Local Minima to a huge Loss\n",
    " <img src=https://i.stack.imgur.com/V9r3G.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = model.generate(encoded_input, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I dont know she said I think Im going to have to go to the hospital I think itll be a long time before I can get back to work She looked up at him then down at his face He looked at her for a moment and then said Dont worry about it Its not like youre going anywhere Well see you in a few days He turned to her and'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## need to remove punctations using string and replace methods\n",
    "## and also check not to repeat the question or the sentecne itself again\n",
    "tokenizer.decode(generator[0], skip_special_tokens=True).translate(str.maketrans(\"\", \"\", string.punctuation)).replace(\"\\n\", \" \")[len(text)+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW EVERTHING IS READY.. WE CAN USE IT IN OUR AI-ASSISTANT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a93485ebf9ca5aa204814fdbfa6a7f3023aa18268526f70d583e64be5730390c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
